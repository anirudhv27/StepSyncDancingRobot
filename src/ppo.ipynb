{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/idanshen/easyrl.git@sac > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym_minigrid\n",
      "  Using cached gym_minigrid-1.2.2-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: gym>=0.26 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym_minigrid) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym_minigrid) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym_minigrid) (3.8.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym>=0.26->gym_minigrid) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym>=0.26->gym_minigrid) (0.0.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib>=3.0->gym_minigrid) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym_minigrid) (1.16.0)\n",
      "Using cached gym_minigrid-1.2.2-py3-none-any.whl (70 kB)\n",
      "Installing collected packages: gym_minigrid\n",
      "Successfully installed gym_minigrid-1.2.2\n",
      "Requirement already satisfied: gym in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: matplotlib in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.14.0\n",
      "  latest version: 24.4.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "Requirement already satisfied: pybullet in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (3.2.6)\n",
      "Collecting torch\n",
      "  Using cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.14.0 fsspec-2024.3.1 mpmath-1.3.0 networkx-3.3 sympy-1.12 torch-2.3.0\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.18.0-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_minigrid\n",
    "!pip install gym\n",
    "!pip install matplotlib\n",
    "!conda install mpi4py\n",
    "!pip install tqdm\n",
    "!pip install pybullet\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pandas\n",
    "!pip install tensorboard\n",
    "#!pip install scikit-learn\n",
    "#!brew install open-mpi\n",
    "#!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can enable the GPU by changing the runtime (Runtime -> Change runtime type -> Hardware accelerator -> GPU )\n",
    "from typing import Tuple, List, Dict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym_minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym\n",
    "import pprint\n",
    "import pybullet as p\n",
    "import pybullet_data as py_d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pylab\n",
    "from gym import spaces\n",
    "from gym.envs.registration import registry, register\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from easyrl.agents.ppo_agent import PPOAgent\n",
    "from easyrl.configs import cfg\n",
    "from easyrl.configs import set_config\n",
    "from easyrl.configs.command_line import cfg_from_cmd\n",
    "from easyrl.engine.ppo_engine import PPOEngine\n",
    "from easyrl.models.categorical_policy import CategoricalPolicy\n",
    "from easyrl.models.diag_gaussian_policy import DiagGaussianPolicy\n",
    "from easyrl.models.mlp import MLP\n",
    "from easyrl.models.value_net import ValueNet\n",
    "from easyrl.runner.nstep_runner import EpisodicRunner\n",
    "from easyrl.utils.common import set_random_seed\n",
    "from easyrl.utils.gym_util import make_vec_env\n",
    "from easyrl.utils.common import load_from_json\n",
    "from base64 import b64encode\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from collections import UserDict\n",
    "import gym.envs.registration\n",
    "\n",
    "registry = UserDict(gym.envs.registration.registry)\n",
    "registry.env_specs = gym.envs.registration.registry\n",
    "gym.envs.registration.registry = registry\n",
    "\n",
    "import pybullet_envs\n",
    "from pybullet_envs.deep_mimic.gym_env.deep_mimic_env import HumanoidDeepBulletEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomHumanoidDeepBulletEnv(HumanoidDeepBulletEnv):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
    "        \n",
    "    def __init__(self, renders=False, arg_file='', test_mode=False,\n",
    "                 time_step=1./240, rescale_actions=True, rescale_observations=True,\n",
    "                 custom_cam_dist=4, custom_cam_pitch=0.1, custom_cam_yaw=45):\n",
    "        \n",
    "        super().__init__(renders=renders, arg_file=arg_file, test_mode=test_mode,\n",
    "                         time_step=time_step, rescale_actions=rescale_actions, \n",
    "                         rescale_observations=rescale_observations)\n",
    "        \n",
    "        self._cam_dist = custom_cam_dist\n",
    "        self._cam_pitch = custom_cam_pitch\n",
    "        self._cam_yaw = custom_cam_yaw\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        if mode == \"human\":\n",
    "            self._renders = True\n",
    "        if mode != \"rgb_array\":\n",
    "            return np.array([])\n",
    "        human = self._internal_env._humanoid\n",
    "        base_pos, orn = self._p.getBasePositionAndOrientation(human._sim_model)\n",
    "        base_pos = np.asarray(base_pos)\n",
    "        # track the position\n",
    "        base_pos[1] += 0.1\n",
    "        rpy = self._p.getEulerFromQuaternion(orn)  # rpy, in radians\n",
    "        rpy = 180 / np.pi * np.asarray(rpy)  # convert rpy in degrees\n",
    "\n",
    "        if (not self._p == None):\n",
    "            view_matrix = self._p.computeViewMatrixFromYawPitchRoll(\n",
    "                cameraTargetPosition=base_pos,\n",
    "                distance=self._cam_dist,\n",
    "                yaw=self._cam_yaw,\n",
    "                pitch=self._cam_pitch,\n",
    "                roll=0,\n",
    "                upAxisIndex=1)\n",
    "            proj_matrix = self._p.computeProjectionMatrixFOV(fov=60,\n",
    "                    aspect=float(self._render_width) / self._render_height,\n",
    "                    nearVal=0.1,\n",
    "                    farVal=100.0)\n",
    "            (_, _, px, _, _) = self._p.getCameraImage(\n",
    "                width=self._render_width,\n",
    "                height=self._render_height,\n",
    "                renderer=self._p.ER_BULLET_HARDWARE_OPENGL,\n",
    "                viewMatrix=view_matrix,\n",
    "                projectionMatrix=proj_matrix)\n",
    "        else:\n",
    "            px = np.array([[[255,255,255,255]]*self._render_width]*self._render_height, dtype=np.uint8)\n",
    "        rgb_array = np.array(px, dtype=np.uint8)\n",
    "        rgb_array = np.reshape(np.array(px), (self._render_height, self._render_width, -1))\n",
    "        rgb_array = rgb_array[:, :, :3]\n",
    "        return rgb_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self,\n",
    "                score_threshold=0.93,\n",
    "                discount=0.995,\n",
    "                lr=1e-3,\n",
    "                max_grad_norm=0.5,\n",
    "                log_interval=10,\n",
    "                max_episodes=500,\n",
    "                gae_lambda=0.95,\n",
    "                use_critic=False,\n",
    "                clip_ratio=0.2,\n",
    "                target_kl=0.01,\n",
    "                train_ac_iters=5,\n",
    "                use_discounted_reward=False,\n",
    "                entropy_coef=0.01,\n",
    "                use_gae=False):\n",
    "\n",
    "        self.score_threshold = score_threshold # criterion for early stopping. If the rolling average reward (over the last 100 episodes) is greater than it, it ends.\n",
    "        self.discount = discount # discount factor\n",
    "        self.lr = lr # learning rate\n",
    "        self.max_grad_norm = max_grad_norm # the maximum gradient norm (https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
    "        self.log_interval = log_interval # logging interval\n",
    "        self.max_episodes = max_episodes # the maximum number of episodes.\n",
    "        self.use_critic = use_critic # whether to use critic or not.\n",
    "        self.clip_ratio = clip_ratio # clip_ratio of PPO.\n",
    "        self.target_kl = target_kl # target KL divergence for early stoping train_ac_iters for PPO\n",
    "        self.train_ac_iters = train_ac_iters # how many time to train ac_model using current computed old_logps\n",
    "        self.gae_lambda=gae_lambda # lambda in Generalized Advantage Estimation (GAE)\n",
    "        self.use_discounted_reward=use_discounted_reward # whether use discounted reward or not.\n",
    "        self.entropy_coef = entropy_coef # entropy coefficient for PPO\n",
    "        self.use_gae = use_gae # whether to use GAE or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: stable-baselines3[extra]\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MlpPolicy' from 'stable_baselines3.common.policies' (/Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages/stable_baselines3/common/policies.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlpPolicy\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO1\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MlpPolicy' from 'stable_baselines3.common.policies' (/Users/raynaarora/.julia/conda/3/envs/stepsync3/lib/python3.12/site-packages/stable_baselines3/common/policies.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3 import PPO1\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = PPO1(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo1_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO1.load(\"ppo1_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACModel(nn.Module):\n",
    "    def __init__(self, use_critic=False):\n",
    "        \"\"\"\n",
    "        Represents an Actor Crictic model that takes a 2d, multi-channeled\n",
    "        image as input.\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        num_actions : int\n",
    "\n",
    "                      The action space of the environment.\n",
    "                      The action space for DoorKey5x5 is 7-dimensional:\n",
    "                      0: turn left,\n",
    "                      1: turn right,\n",
    "                      2: forward,\n",
    "                      3: pickup an object,\n",
    "                      4: drop an object,\n",
    "                      5: activate an object,\n",
    "                      6: done completing task\n",
    "\n",
    "        use_critics : bool\n",
    "\n",
    "                      Critic network will be used in forward pass if flag is set\n",
    "                      to true.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_critic = use_critic\n",
    "\n",
    "        low = np.array([-5, -5, -5, -5, -5])\n",
    "        high = -np.array([-5, -5, -5, -5, -5])\n",
    "        self.observation_space = gym.spaces.Box(low, high, dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Define actor's model\n",
    "        self.image_conv_actor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        if self.use_critic:\n",
    "            self.image_conv_critic = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d((2, 2)),\n",
    "                nn.Conv2d(16, 32, (2, 2)),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, (2, 2)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(64, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the actor-critic network\n",
    "\n",
    "        Parameters\n",
    "        ----\n",
    "        obs : int tensor. Shape [Batch size, ImWidth, ImHeight, Channels]\n",
    "\n",
    "              input to the network.\n",
    "        ----\n",
    "\n",
    "        returns:\n",
    "\n",
    "        dist : torch.distribution\n",
    "            The distribution of actions from policy. A Categorical distribution\n",
    "            for discreet action spaces.\n",
    "        value : torch.Tensor (Batch size, 1)\n",
    "            value output by critic network\n",
    "        \"\"\"\n",
    "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into expected order\n",
    "\n",
    "        dist, value = None, None\n",
    "\n",
    "        x = self.image_conv_actor(conv_in)\n",
    "        embedding = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "\n",
    "        if self.use_critic:\n",
    "            y = self.image_conv_critic(conv_in)\n",
    "            embedding = y.reshape(y.shape[0], -1)\n",
    "\n",
    "            value = self.critic(embedding).squeeze(1)\n",
    "        else:\n",
    "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
    "\n",
    "        return dist, value\n",
    "\n",
    "def compute_discounted_return(rewards, discount, device=None):\n",
    "    \"\"\"\n",
    "\t\trewards: reward obtained at timestep.  Shape: (T,)\n",
    "\t\tdiscount: discount factor. float\n",
    "\n",
    "    ----\n",
    "    returns: sum of discounted rewards. Shape: (T,)\n",
    "\t\t\"\"\"\n",
    "    returns = torch.zeros(*rewards.shape, device=device)\n",
    "\n",
    "    R = 0\n",
    "    for t in reversed(range((rewards.shape[0]))):\n",
    "        R = rewards[t] + discount * R\n",
    "        returns[t] = R\n",
    "    return returns\n",
    "\n",
    "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
    "    \"\"\"\n",
    "    Compute Adavantage wiht GAE. See Section 4.4.2 in the lecture notes.\n",
    "\n",
    "    values: value at each timestep (T,)\n",
    "    rewards: reward obtained at each timestep.  Shape: (T,)\n",
    "    T: the number of frames, float\n",
    "    gae_lambda: hyperparameter, float\n",
    "    discount: discount factor, float\n",
    "\n",
    "    -----\n",
    "\n",
    "    returns:\n",
    "\n",
    "    advantages : tensor.float. Shape [T,]\n",
    "\n",
    "                 gae advantage term for timesteps 0 to T\n",
    "\n",
    "    \"\"\"\n",
    "    advantages = torch.zeros_like(values)\n",
    "    for i in reversed(range(T)):\n",
    "        next_value = values[i+1]\n",
    "        next_advantage = advantages[i+1]\n",
    "\n",
    "        delta = rewards[i] + args.discount * next_value  - values[i]\n",
    "        advantages[i] = delta + args.discount * args.gae_lambda * next_advantage\n",
    "    return advantages[:T]\n",
    "\n",
    "def collect_experiences(env, acmodel, args, device=None):\n",
    "    \"\"\"Collects rollouts and computes advantages.\n",
    "    Returns\n",
    "    -------\n",
    "    exps : dict\n",
    "        Contains actions, rewards, advantages etc as attributes.\n",
    "        Each attribute, e.g. `exps['reward']` has a shape\n",
    "        (self.num_frames, ...).\n",
    "    logs : dict\n",
    "        Useful stats about the training process, including the average\n",
    "        reward, policy loss, value loss, etc.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    MAX_FRAMES_PER_EP = 300\n",
    "    shape = (MAX_FRAMES_PER_EP, )\n",
    "\n",
    "    actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
    "    values = torch.zeros(*shape, device=device)\n",
    "    rewards = torch.zeros(*shape, device=device)\n",
    "    log_probs = torch.zeros(*shape, device=device)\n",
    "    obss = [None]*MAX_FRAMES_PER_EP\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    total_return = 0\n",
    "\n",
    "    T = 0\n",
    "\n",
    "    while True:\n",
    "        # Do one agent-environment interaction\n",
    "\n",
    "        preprocessed_obs = preprocess_obss(obs, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dist, value = acmodel(preprocessed_obs)\n",
    "        action = dist.sample()[0]\n",
    "\n",
    "\n",
    "        obss[T] = obs\n",
    "        obs, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "\n",
    "        # Update experiences values\n",
    "        actions[T] = action\n",
    "        values[T] = value\n",
    "        rewards[T] = reward\n",
    "        log_probs[T] = dist.log_prob(action)\n",
    "\n",
    "\n",
    "        total_return += reward\n",
    "        T += 1\n",
    "\n",
    "        if done or T>=MAX_FRAMES_PER_EP-1:\n",
    "            break\n",
    "\n",
    "    discounted_reward = compute_discounted_return(rewards[:T], args.discount, device)\n",
    "    exps = dict(\n",
    "        obs = preprocess_obss([\n",
    "            obss[i]\n",
    "            for i in range(T)\n",
    "        ], device=device),\n",
    "        action = actions[:T],\n",
    "        value  = values[:T],\n",
    "        reward = rewards[:T],\n",
    "        advantage = discounted_reward-values[:T],\n",
    "        log_prob = log_probs[:T],\n",
    "        discounted_reward = discounted_reward,\n",
    "        advantage_gae=compute_advantage_gae(values, rewards, T, args.gae_lambda, args.discount)\n",
    "    )\n",
    "\n",
    "    logs = {\n",
    "        \"return_per_episode\": total_return,\n",
    "        \"num_frames\": T\n",
    "    }\n",
    "\n",
    "    return exps, logs\n",
    "\n",
    "def run_experiment(args, parameter_update, seed=0):\n",
    "    \"\"\"\n",
    "    Upper level function for running experiments to analyze reinforce and\n",
    "    policy gradient methods. Instantiates a model, collects epxeriences, and\n",
    "    then updates the neccessary parameters.\n",
    "\n",
    "    args: Config arguments. dict\n",
    "    paramter_update: function used to update model parameters\n",
    "    seed: random seed. int\n",
    "\n",
    "    return: DataFrame indexed by episode\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = DoorKeyEnv5x5()\n",
    "\n",
    "    acmodel = ACModel(env.action_space.n, use_critic=args.use_critic)\n",
    "    acmodel.to(device)\n",
    "\n",
    "    is_solved = False\n",
    "\n",
    "    SMOOTH_REWARD_WINDOW = 50\n",
    "\n",
    "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
    "\n",
    "    optimizer = torch.optim.Adam(acmodel.parameters(), lr=args.lr)\n",
    "    num_frames = 0\n",
    "\n",
    "    pbar = tqdm(range(args.max_episodes))\n",
    "    for update in pbar:\n",
    "        exps, logs1 = collect_experiences(env, acmodel, args, device)\n",
    "        logs2 = parameter_update(optimizer, acmodel, exps, args)\n",
    "\n",
    "        logs = {**logs1, **logs2}\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "\n",
    "        rewards.append(logs[\"return_per_episode\"])\n",
    "\n",
    "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
    "\n",
    "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
    "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
    "\n",
    "        if args.use_critic:\n",
    "            data['value_loss'] = logs[\"value_loss\"]\n",
    "\n",
    "        pd_logs.append(data)\n",
    "\n",
    "        pbar.set_postfix(data)\n",
    "\n",
    "        # Early terminate\n",
    "        if smooth_reward >= args.score_threshold:\n",
    "            is_solved = True\n",
    "            break\n",
    "\n",
    "    if is_solved:\n",
    "        print('Solved!')\n",
    "\n",
    "    return pd.DataFrame(pd_logs).set_index('episode')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stepsync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
